{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "DATA_PATH = \"../../data/\"\n",
    "\n",
    "books_data = pd.read_csv(f\"{DATA_PATH}/raw/books_data.csv\")\n",
    "cleaned_reviews = pd.read_csv(f\"{DATA_PATH}/cleaned_data/cleaned_reviews.csv\")\n",
    "\n",
    "with open(f\"{DATA_PATH}/embeddings/bert_embeddings.npy\", \"rb\") as f:\n",
    "    bert_embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded the dataframe as cleaned_reviews\n",
    "# Filter out users with less than a minimum number of reviews\n",
    "MIN_REVIEWS = 3\n",
    "user_counts = cleaned_reviews['User_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= MIN_REVIEWS].index\n",
    "\n",
    "filtered_reviews = cleaned_reviews.copy()[cleaned_reviews['User_id'].isin(valid_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/5vmyzh215993g2001_cz3wjc0000gp/T/ipykernel_4907/3882211436.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  filtered_reviews['normalized_score'] = filtered_reviews.apply(lambda row: (row['review/score'] - user_stats.loc[row['User_id'], 'mean']) / user_stats.loc[row['User_id'], 'std'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean and std per user for rescaling\n",
    "user_stats = filtered_reviews.groupby('User_id')['review/score'].agg(['mean', 'std'])\n",
    "\n",
    "# Apply rescaling per user\n",
    "filtered_reviews['normalized_score'] = filtered_reviews.apply(lambda row: (row['review/score'] - user_stats.loc[row['User_id'], 'mean']) / user_stats.loc[row['User_id'], 'std'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_nan_norm_score = filtered_reviews[filtered_reviews['normalized_score'].isna()].User_id.unique()\n",
    "filtered_reviews = filtered_reviews[~filtered_reviews.User_id.isin(users_with_nan_norm_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where the key is the title and the value is its index in the books_data DataFrame\n",
    "title_to_index = {title: idx for idx, title in enumerate(books_data['Title'])}\n",
    "\n",
    "# Map the Title in filtered_reviews to its index in books_data\n",
    "filtered_reviews = filtered_reviews.copy()\n",
    "filtered_reviews['Title_Index'] = filtered_reviews['Title'].map(title_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by user and time\n",
    "filtered_reviews = filtered_reviews.sort_values(by=['User_id', 'review/time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>normalized_score</th>\n",
       "      <th>Title_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17852</th>\n",
       "      <td>B0000CJ9GZ</td>\n",
       "      <td>The richest man in Babylon</td>\n",
       "      <td>A0015610VMNR0JC9XVL1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17939</th>\n",
       "      <td>0785263500</td>\n",
       "      <td>ATTITUDE 101</td>\n",
       "      <td>A0015610VMNR0JC9XVL1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>-2.041241</td>\n",
       "      <td>119672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17972</th>\n",
       "      <td>B000GQMVWI</td>\n",
       "      <td>The Richest Man in Babylon</td>\n",
       "      <td>A0015610VMNR0JC9XVL1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>51729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>B0007DRIT6</td>\n",
       "      <td>The richest man in Babylon</td>\n",
       "      <td>A0015610VMNR0JC9XVL1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18135</th>\n",
       "      <td>B0007G66WI</td>\n",
       "      <td>The richest man in Babylon</td>\n",
       "      <td>A0015610VMNR0JC9XVL1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358987</th>\n",
       "      <td>B000OTPXI6</td>\n",
       "      <td>Our Yanks a Love Story</td>\n",
       "      <td>AZZVZL4QEHEHO</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1201824000</td>\n",
       "      <td>0.225387</td>\n",
       "      <td>175010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359126</th>\n",
       "      <td>0552148229</td>\n",
       "      <td>Our Yanks</td>\n",
       "      <td>AZZVZL4QEHEHO</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1201824000</td>\n",
       "      <td>0.225387</td>\n",
       "      <td>164117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122469</th>\n",
       "      <td>B000GSQ910</td>\n",
       "      <td>Redeeming Love</td>\n",
       "      <td>AZZVZL4QEHEHO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1322784000</td>\n",
       "      <td>1.030339</td>\n",
       "      <td>127021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107493</th>\n",
       "      <td>0553563793</td>\n",
       "      <td>Long Night Moon</td>\n",
       "      <td>AZZVZL4QEHEHO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1329523200</td>\n",
       "      <td>-0.579566</td>\n",
       "      <td>145343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102944</th>\n",
       "      <td>0553563785</td>\n",
       "      <td>One Fine Day</td>\n",
       "      <td>AZZVZL4QEHEHO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1331424000</td>\n",
       "      <td>-0.579566</td>\n",
       "      <td>145207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>930549 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id                       Title               User_id  \\\n",
       "17852   B0000CJ9GZ  The richest man in Babylon  A0015610VMNR0JC9XVL1   \n",
       "17939   0785263500                ATTITUDE 101  A0015610VMNR0JC9XVL1   \n",
       "17972   B000GQMVWI  The Richest Man in Babylon  A0015610VMNR0JC9XVL1   \n",
       "18079   B0007DRIT6  The richest man in Babylon  A0015610VMNR0JC9XVL1   \n",
       "18135   B0007G66WI  The richest man in Babylon  A0015610VMNR0JC9XVL1   \n",
       "...            ...                         ...                   ...   \n",
       "358987  B000OTPXI6      Our Yanks a Love Story         AZZVZL4QEHEHO   \n",
       "359126  0552148229                   Our Yanks         AZZVZL4QEHEHO   \n",
       "122469  B000GSQ910              Redeeming Love         AZZVZL4QEHEHO   \n",
       "107493  0553563793             Long Night Moon         AZZVZL4QEHEHO   \n",
       "102944  0553563785                One Fine Day         AZZVZL4QEHEHO   \n",
       "\n",
       "        review/score  review/time  normalized_score  Title_Index  \n",
       "17852            5.0   1358985600          0.408248       105455  \n",
       "17939            3.0   1358985600         -2.041241       119672  \n",
       "17972            5.0   1358985600          0.408248        51729  \n",
       "18079            5.0   1358985600          0.408248       105455  \n",
       "18135            5.0   1358985600          0.408248       105455  \n",
       "...              ...          ...               ...          ...  \n",
       "358987           4.0   1201824000          0.225387       175010  \n",
       "359126           4.0   1201824000          0.225387       164117  \n",
       "122469           5.0   1322784000          1.030339       127021  \n",
       "107493           3.0   1329523200         -0.579566       145343  \n",
       "102944           3.0   1331424000         -0.579566       145207  \n",
       "\n",
       "[930549 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47333/47333 [26:17<00:00, 30.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15778/15778 [09:09<00:00, 28.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15778/15778 [09:11<00:00, 28.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_REVIEWS = 10\n",
    "SCORE_COLUMN = \"review/score\"\n",
    "\n",
    "# Split the unique user_ids into train, validation and test sets\n",
    "train_users, temp_users = train_test_split(filtered_reviews['User_id'].unique(), test_size=0.4, random_state=42)\n",
    "valid_users, test_users = train_test_split(temp_users, test_size=0.5, random_state=42)\n",
    "\n",
    "def prepare_sequences(users, df):\n",
    "    sequences = []\n",
    "    for user in tqdm(users):\n",
    "        user_data = df[df['User_id'] == user]\n",
    "        \n",
    "        # Ensure that we keep only the most recent MAX_REVIEWS reviews\n",
    "        if len(user_data) > MAX_REVIEWS + 1:\n",
    "            user_data = user_data.iloc[-(MAX_REVIEWS + 1):]  # +1 to account for the query doc\n",
    "\n",
    "        title_indexes = user_data['Title_Index'].values[:-1]\n",
    "        scores = user_data[SCORE_COLUMN].values[:-1]\n",
    "        \n",
    "        mask = [1] * len(title_indexes) + [0] * (MAX_REVIEWS - len(title_indexes))\n",
    "        title_indexes = list(title_indexes) + [0] * (MAX_REVIEWS - len(title_indexes))\n",
    "        scores = list(scores) + [0] * (MAX_REVIEWS - len(scores))\n",
    "        query_id = user_data['Title_Index'].values[-1]\n",
    "        target = user_data[SCORE_COLUMN].values[-1]\n",
    "        \n",
    "        sequences.append([title_indexes, scores, mask, query_id, target])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "train_sequences = prepare_sequences(train_users, filtered_reviews)\n",
    "valid_sequences = prepare_sequences(valid_users, filtered_reviews)\n",
    "test_sequences = prepare_sequences(test_users, filtered_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title_indexes, scores, mask, query_id, target = self.sequences[idx]\n",
    "        return torch.tensor(title_indexes, dtype=torch.long), \\\n",
    "               torch.tensor(scores, dtype=torch.float32), \\\n",
    "               torch.tensor(mask, dtype=torch.float32), \\\n",
    "               torch.tensor(query_id, dtype=torch.long), \\\n",
    "               torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "train_dataset = ReviewDataset(train_sequences)\n",
    "valid_dataset = ReviewDataset(valid_sequences)\n",
    "test_dataset = ReviewDataset(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  5956, 212159, 202391,      0,      0,      0,      0,      0,      0,\n",
       "              0]),\n",
       " tensor([5., 4., 3., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor(203884),\n",
       " tensor(5.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UserBehaviorEncoder(nn.Module):\n",
    "    def __init__(self, embeddings, num_heads, output_dim):\n",
    "        super(UserBehaviorEncoder, self).__init__()\n",
    "        \n",
    "        self.doc_emb_dim = embeddings.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings).float(), padding_idx=0, freeze=True)\n",
    "        \n",
    "        # Embedding to transform scores to the same dimensionality as document embeddings\n",
    "        self.score_emb = nn.Linear(1, self.doc_emb_dim)\n",
    "        \n",
    "        # MultiHeadAttention Layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.doc_emb_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Dense layer to produce final user behavior representation\n",
    "        self.fc = nn.Linear(self.doc_emb_dim, output_dim)\n",
    "\n",
    "    def forward(self, doc_indices, scores, attention_mask=None):\n",
    "        # Get the document embeddings using the indices\n",
    "        doc_embeddings = self.embedding(doc_indices)\n",
    "        \n",
    "        # Transform scores to embeddings\n",
    "        scores_emb = self.score_emb(scores.unsqueeze(-1)).squeeze(2)  # Add an extra dimension to the last axis and then squeeze it out\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_emb = doc_embeddings + scores_emb\n",
    "\n",
    "        # Permute to match expected shape\n",
    "        combined_emb_permuted = combined_emb.permute(1, 0, 2)\n",
    "        \n",
    "        # Apply the attention mechanism\n",
    "        attended_values_permuted, _ = self.attention(\n",
    "            combined_emb_permuted,\n",
    "            combined_emb_permuted,\n",
    "            combined_emb_permuted,\n",
    "            key_padding_mask=(attention_mask == 0).bool()\n",
    "        )\n",
    "\n",
    "        # Permute the values back to (batch_size, sequence_length, embedding_dim)\n",
    "        attended_values = attended_values_permuted.permute(1, 0, 2)\n",
    "        \n",
    "        # Produce user behavior embedding\n",
    "        user_behavior_emb = attended_values.mean(dim=1)\n",
    "        \n",
    "        # Pass through a dense layer\n",
    "        user_behavior_rep = self.fc(user_behavior_emb)\n",
    "        \n",
    "        return user_behavior_rep\n",
    "\n",
    "class UserPreferenceRegressor(nn.Module):\n",
    "    def __init__(self, embeddings, num_heads, user_behavior_output_dim, predictor_hidden_dim):\n",
    "        super(UserPreferenceRegressor, self).__init__()\n",
    "        \n",
    "        self.encoder = UserBehaviorEncoder(embeddings, num_heads, user_behavior_output_dim)\n",
    "        \n",
    "        # Define the predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(user_behavior_output_dim + embeddings.shape[1], predictor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(predictor_hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, doc_indices, scores, attention_mask, new_doc_embedding):\n",
    "        user_behavior = self.encoder(doc_indices, scores, attention_mask)\n",
    "        combined = torch.cat([user_behavior, new_doc_embedding], dim=1)\n",
    "        return self.predictor(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "NUM_HEADS = 2  # for MultiHeadAttention\n",
    "OUTPUT_DIM = 128  # User behavior representation dimension\n",
    "HIDDEN_DIM = 256  # Predictor hidden layer dimension\n",
    "\n",
    "# Initialize model\n",
    "model = UserPreferenceRegressor(bert_embeddings, NUM_HEADS, OUTPUT_DIM, HIDDEN_DIM)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.7129\n",
      "Validation Loss: 2.3008\n",
      "Epoch [2/100], Train Loss: 1.6166\n",
      "Validation Loss: 1.6528\n",
      "Epoch [3/100], Train Loss: 1.6163\n",
      "Validation Loss: 1.5950\n",
      "Epoch [4/100], Train Loss: 1.6035\n",
      "Validation Loss: 1.5990\n",
      "Epoch [5/100], Train Loss: 4.1110\n",
      "Validation Loss: 1.6286\n",
      "Epoch [6/100], Train Loss: 1.5241\n",
      "Validation Loss: 1.5363\n",
      "Epoch [7/100], Train Loss: 1.5047\n",
      "Validation Loss: 1.4903\n",
      "Epoch [8/100], Train Loss: 1.5319\n",
      "Validation Loss: 1.4735\n",
      "Epoch [9/100], Train Loss: 1.5665\n",
      "Validation Loss: 1.5696\n",
      "Epoch [10/100], Train Loss: 1.5510\n",
      "Validation Loss: 1.5247\n",
      "Epoch [11/100], Train Loss: 1.5611\n",
      "Validation Loss: 1.5898\n",
      "Epoch [12/100], Train Loss: 1.5354\n",
      "Validation Loss: 1.6701\n",
      "Epoch [13/100], Train Loss: 1.5063\n",
      "Validation Loss: 1.4764\n",
      "Epoch [14/100], Train Loss: 1.4940\n",
      "Validation Loss: 1.5019\n",
      "Epoch [15/100], Train Loss: 1.4915\n",
      "Validation Loss: 1.4820\n",
      "Epoch [16/100], Train Loss: 1.4770\n",
      "Validation Loss: 1.4902\n",
      "Epoch [17/100], Train Loss: 1.4743\n",
      "Validation Loss: 1.4580\n",
      "Epoch [18/100], Train Loss: 1.4618\n",
      "Validation Loss: 1.4576\n",
      "Epoch [19/100], Train Loss: 1.4648\n",
      "Validation Loss: 1.4979\n",
      "Epoch [20/100], Train Loss: 1.4521\n",
      "Validation Loss: 1.4533\n",
      "Epoch [21/100], Train Loss: 1.4513\n",
      "Validation Loss: 1.5331\n",
      "Epoch [22/100], Train Loss: 1.4419\n",
      "Validation Loss: 1.6420\n",
      "Epoch [23/100], Train Loss: 1.4464\n",
      "Validation Loss: 1.4612\n",
      "Epoch [24/100], Train Loss: 1.4312\n",
      "Validation Loss: 1.4599\n",
      "Epoch [25/100], Train Loss: 1.4318\n",
      "Validation Loss: 1.4442\n",
      "Epoch [26/100], Train Loss: 1.4293\n",
      "Validation Loss: 1.5402\n",
      "Epoch [27/100], Train Loss: 1.4186\n",
      "Validation Loss: 1.4645\n",
      "Epoch [28/100], Train Loss: 1.4176\n",
      "Validation Loss: 1.4698\n",
      "Epoch [29/100], Train Loss: 1.4116\n",
      "Validation Loss: 1.4460\n",
      "Epoch [30/100], Train Loss: 1.4017\n",
      "Validation Loss: 1.4240\n",
      "Epoch [31/100], Train Loss: 1.3814\n",
      "Validation Loss: 1.4823\n",
      "Epoch [32/100], Train Loss: 1.4154\n",
      "Validation Loss: 1.4348\n",
      "Epoch [33/100], Train Loss: 1.3779\n",
      "Validation Loss: 1.4503\n",
      "Epoch [34/100], Train Loss: 1.3793\n",
      "Validation Loss: 1.4251\n",
      "Epoch [35/100], Train Loss: 1.3720\n",
      "Validation Loss: 1.4530\n",
      "Epoch [36/100], Train Loss: 1.3730\n",
      "Validation Loss: 1.4177\n",
      "Epoch [37/100], Train Loss: 1.3672\n",
      "Validation Loss: 1.4257\n",
      "Epoch [38/100], Train Loss: 1.4095\n",
      "Validation Loss: 1.4340\n",
      "Epoch [39/100], Train Loss: 1.3881\n",
      "Validation Loss: 1.4066\n",
      "Epoch [40/100], Train Loss: 1.3683\n",
      "Validation Loss: 1.4045\n",
      "Epoch [41/100], Train Loss: 1.3582\n",
      "Validation Loss: 1.4418\n",
      "Epoch [42/100], Train Loss: 1.3628\n",
      "Validation Loss: 1.4137\n",
      "Epoch [43/100], Train Loss: 1.3542\n",
      "Validation Loss: 1.4424\n",
      "Epoch [44/100], Train Loss: 1.3576\n",
      "Validation Loss: 1.4332\n",
      "Epoch [45/100], Train Loss: 1.3584\n",
      "Validation Loss: 1.4026\n",
      "Epoch [46/100], Train Loss: 1.3563\n",
      "Validation Loss: 1.4069\n",
      "Epoch [47/100], Train Loss: 1.3458\n",
      "Validation Loss: 1.4237\n",
      "Epoch [48/100], Train Loss: 1.3475\n",
      "Validation Loss: 1.4171\n"
     ]
    }
   ],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for idx, (title_indexes, scores, mask, query_id, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            new_doc_embedding = model.encoder.embedding(query_id).squeeze(1)\n",
    "            outputs = model(title_indexes, scores.unsqueeze(2), mask, new_doc_embedding)\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(1), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for title_indexes, scores, mask, query_id, target in valid_loader:\n",
    "                new_doc_embedding = model.encoder.embedding(query_id).squeeze(1)\n",
    "                outputs = model(title_indexes, scores.unsqueeze(2), mask, new_doc_embedding)\n",
    "                loss = criterion(outputs.squeeze(1), target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss/len(valid_loader):.4f}\")\n",
    "        \n",
    "# Call the training function\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212404/212404 [01:11<00:00, 2986.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict_scores_for_all_books(model, embeddings, doc_indices, scores):\n",
    "    \"\"\"\n",
    "    Predict the scores for a user for all N books.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained UserPreferenceRegressor model.\n",
    "    - embeddings: The embeddings for all N books.\n",
    "    - doc_indices: A list of document indices representing user reviews.\n",
    "    - scores: A list of scores associated with the user reviews.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples with each tuple containing (book_id, predicted_score).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the doc_indices and scores to tensors\n",
    "    doc_indices_tensor = torch.tensor([doc_indices + [0] * (10 - len(doc_indices))], dtype=torch.long)\n",
    "    scores_tensor = torch.tensor([scores + [0.0] * (MAX_REVIEWS - len(scores))], dtype=torch.float).unsqueeze(-1)\n",
    "    \n",
    "    # Create an attention mask for valid reviews\n",
    "    attention_mask = [False] * len(doc_indices) + [True] * (MAX_REVIEWS - len(doc_indices))\n",
    "    attention_mask_tensor = torch.tensor([attention_mask])\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    # Loop over all book embeddings to predict the scores\n",
    "    for book_id in tqdm(range(embeddings.shape[0])):\n",
    "        # Get the book embedding tensor\n",
    "        new_doc_embedding_tensor = torch.tensor(embeddings[book_id]).unsqueeze(0)\n",
    "\n",
    "        # Use the model to predict the score\n",
    "        predicted_score_tensor = model(doc_indices_tensor, scores_tensor, attention_mask_tensor, new_doc_embedding_tensor)\n",
    "        \n",
    "        # Convert tensor to scalar and append to all_predictions list\n",
    "        all_predictions.append(predicted_score_tensor.item())\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "preds = predict_scores_for_all_books(model, bert_embeddings, [81605, ], [5., ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = pd.Series(preds).sort_values(ascending=False)\n",
    "top_results.name = \"score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = pd.merge(top_results, books_data, left_index=True, right_index=True)[[\"score\",\"Title\",\"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132571</th>\n",
       "      <td>4.613784</td>\n",
       "      <td>The People Came (To Osborne County, Kansas) in Their Prairie Schooners Through the Waves of the Seas of Grass and Stayed (Volume 1)</td>\n",
       "      <td>The first volume in the series State Bibliographies, this book provides comprehensive coverage of secondary materials on Kansas history and also includes useful references to major archival and manuscript collections. Its broad and diverse scope ranges from standard political and economic studies to social and environmental histories, to local studies, to regional studies with special signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153834</th>\n",
       "      <td>4.533461</td>\n",
       "      <td>No tears for the general;: The life of Alfred Sully, 1821-1879 (Western biography series)</td>\n",
       "      <td>\"Letters of Sully, printed for the first time, provide a vivid picture of California in the gold rush, of Minnesota frontier in the 1850s, Civil War, Sioux uprising, etc.\"--Bookseller's catalogue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174319</th>\n",
       "      <td>4.510740</td>\n",
       "      <td>Come and git' it!</td>\n",
       "      <td>In this colorful tale of the food culture of cattle drives in the 1800s, chuckwagon chef Cookie takes young readers along on a rootin'-tootin' adventure. The days start at three o'clock in the morning, when Cookie makes coffee so thick \"you could float a horseshoe on it!\" With informational sidebars, a historical note, bibliography, and glossary for cowboy food terms such as \"calf slobbers\" an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32008</th>\n",
       "      <td>4.494857</td>\n",
       "      <td>Fifty famous farmers,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46496</th>\n",
       "      <td>4.474873</td>\n",
       "      <td>Landmarked: Stories of Peggy Simson Curry</td>\n",
       "      <td>Peggy Simson Curry's memorable short stories, many set on Wyoming and Colorado ranches, originally appeared in magazines during the heyday of short stories in the 1950s-60s. Now they are available in book form. This collection includes two Spur winners and stories from Saturday Evening Post, Boy's Life and other magazines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93031</th>\n",
       "      <td>4.464562</td>\n",
       "      <td>Just Fishing</td>\n",
       "      <td>The catching of fish, said the Sage of Chocoloskee, is but an incident in fishing. He told the frozen truth. To be out in the open where fish are; to watch them at their great business of living; to see them in the water or out of the water; to fish for them, and even to hook them and have them get away-all this is wonderfully worthwhile-wonderfully better worthwhile than merely to catch and k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86331</th>\n",
       "      <td>4.462640</td>\n",
       "      <td>The Kickapoos: Lords of the Middle Border</td>\n",
       "      <td>The Kickapoo Indians, members of the Algonquian linguistic community, resisted white settlement for more than three hundred years on a front that extended across half a continent. In turn, France, Great Britain, the United States, Spain, and Mexico sought to placate and exploit this fiercely independent people. Eventually forced to remove from their historic homeland to territory west of the M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>4.458817</td>\n",
       "      <td>OF CABBAGES AND KINGS COUNTY: AGRICULTURE AND THE FORMATION OF MODERN BROOKLYN.</td>\n",
       "      <td>No one today thinks of Brooklyn, New York, as an agricultural center. Yet Kings County enjoyed over two centuries of farming prosperity. Even as late as 1880 it was one of the nation's leading vegetable producers, second only to neighboring Queens County. In Of Cabbages and Kings County, Marc Linder and Lawrence Zacharias reconstruct the history of a lost agricultural community. Their study fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59841</th>\n",
       "      <td>4.457653</td>\n",
       "      <td>The Quotable Farmer (Quotable Series)</td>\n",
       "      <td>From the gentlemen farmers among our founders to todayâ€™s grandchild walking the beans, rural life has always been at the very heart of the American story. This is the life that unfolds page by page in this heartfelt book about working the land. One remarkable photograph after another celebrates the farming life, finding the beauty in work well done, land well tended, and a rest well earnedâ€”all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84369</th>\n",
       "      <td>4.455821</td>\n",
       "      <td>Glory days of logging</td>\n",
       "      <td>The reissue of this classic history allows us to once again journey into the past and rediscover for the first time the forgotten men and methods of logging history in the Northwest United States and Canada. This book contain the best photographs of a dozen famous collections: Davis and Benson rafts, river drives, hand logging spar topping big wheels in the pine, saw mills of 1890 to 1915, his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>4.444587</td>\n",
       "      <td>Sea Runners</td>\n",
       "      <td>In 1853, four Scandinavian indentured laborers in Russian Alaska steal a canoe and begin to paddle south toward the mouth of the Columbia River, twelve thousand miles away. Reprint. 15,000 first printing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141633</th>\n",
       "      <td>4.439794</td>\n",
       "      <td>Single Wheeler Locomotives: The Brief Age of Perfection, 1885-1900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112638</th>\n",
       "      <td>4.424620</td>\n",
       "      <td>Classic Farm Tractors: History of the Farm Tractor</td>\n",
       "      <td>The story of the development of the farm tractor with more than 250 beautiful photographs of tractors from across the United States.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51225</th>\n",
       "      <td>4.423578</td>\n",
       "      <td>Logging to the Salt Chuck: Over 100 Years of Railroad Logging in Mason County Washington (Logging Railroads of Washington State Series)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203859</th>\n",
       "      <td>4.412395</td>\n",
       "      <td>Gas Chromatography and Lipids: A Practical Guide</td>\n",
       "      <td>Fatty acids and lipids: structures, extraction and fractionation into classes -- Gas chromatography: theoretical aspects and instrumentation -- Preparation of methyl ester and other derivatives -- Gas chromatographic analysis of fatty acid derivatives -- Isolation of fatty acids and identification by spectroscopic and chemical degradative techniques -- Gas chromatography--mass spectrometry and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30900</th>\n",
       "      <td>4.410502</td>\n",
       "      <td>The St. Louis Cardinals: An Illustrated History</td>\n",
       "      <td>Photographs show over one hundred years in the history of the St. Louis baseball team, from the late 1800s to the present day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35010</th>\n",
       "      <td>4.408042</td>\n",
       "      <td>Farmers of forty centuries;: Or, Permanent agriculture in China, Korea, and Japan,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208570</th>\n",
       "      <td>4.407622</td>\n",
       "      <td>1908 Sears, Roebuck Catalogue: A Treasured Replica from the Archives of History</td>\n",
       "      <td>Take a trip \"south of the border\" with this arrangement of a classic Mexican folk song. Schaum keeps the piece at the late elementary level, even though it sounds quite a bit harder. This piece is a crowd-pleaser, effective for festivals and competitions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80100</th>\n",
       "      <td>4.403210</td>\n",
       "      <td>Taxation: The People's Business</td>\n",
       "      <td>Published in 1924, this volume contains various views on taxation within the United States.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15675</th>\n",
       "      <td>4.401734</td>\n",
       "      <td>The management of common land in north west Europe, c. 1500-1850. (COMPARATIVE RURAL HISTORY OF THE NORTH SEA AREA)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           score  \\\n",
       "132571  4.613784   \n",
       "153834  4.533461   \n",
       "174319  4.510740   \n",
       "32008   4.494857   \n",
       "46496   4.474873   \n",
       "93031   4.464562   \n",
       "86331   4.462640   \n",
       "5541    4.458817   \n",
       "59841   4.457653   \n",
       "84369   4.455821   \n",
       "879     4.444587   \n",
       "141633  4.439794   \n",
       "112638  4.424620   \n",
       "51225   4.423578   \n",
       "203859  4.412395   \n",
       "30900   4.410502   \n",
       "35010   4.408042   \n",
       "208570  4.407622   \n",
       "80100   4.403210   \n",
       "15675   4.401734   \n",
       "\n",
       "                                                                                                                                          Title  \\\n",
       "132571      The People Came (To Osborne County, Kansas) in Their Prairie Schooners Through the Waves of the Seas of Grass and Stayed (Volume 1)   \n",
       "153834                                                No tears for the general;: The life of Alfred Sully, 1821-1879 (Western biography series)   \n",
       "174319                                                                                                                        Come and git' it!   \n",
       "32008                                                                                                                     Fifty famous farmers,   \n",
       "46496                                                                                                 Landmarked: Stories of Peggy Simson Curry   \n",
       "93031                                                                                                                              Just Fishing   \n",
       "86331                                                                                                 The Kickapoos: Lords of the Middle Border   \n",
       "5541                                                            OF CABBAGES AND KINGS COUNTY: AGRICULTURE AND THE FORMATION OF MODERN BROOKLYN.   \n",
       "59841                                                                                                     The Quotable Farmer (Quotable Series)   \n",
       "84369                                                                                                                     Glory days of logging   \n",
       "879                                                                                                                                 Sea Runners   \n",
       "141633                                                                       Single Wheeler Locomotives: The Brief Age of Perfection, 1885-1900   \n",
       "112638                                                                                       Classic Farm Tractors: History of the Farm Tractor   \n",
       "51225   Logging to the Salt Chuck: Over 100 Years of Railroad Logging in Mason County Washington (Logging Railroads of Washington State Series)   \n",
       "203859                                                                                         Gas Chromatography and Lipids: A Practical Guide   \n",
       "30900                                                                                           The St. Louis Cardinals: An Illustrated History   \n",
       "35010                                                        Farmers of forty centuries;: Or, Permanent agriculture in China, Korea, and Japan,   \n",
       "208570                                                          1908 Sears, Roebuck Catalogue: A Treasured Replica from the Archives of History   \n",
       "80100                                                                                                           Taxation: The People's Business   \n",
       "15675                       The management of common land in north west Europe, c. 1500-1850. (COMPARATIVE RURAL HISTORY OF THE NORTH SEA AREA)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                            description  \n",
       "132571  The first volume in the series State Bibliographies, this book provides comprehensive coverage of secondary materials on Kansas history and also includes useful references to major archival and manuscript collections. Its broad and diverse scope ranges from standard political and economic studies to social and environmental histories, to local studies, to regional studies with special signific...  \n",
       "153834                                                                                                                                                                                                             \"Letters of Sully, printed for the first time, provide a vivid picture of California in the gold rush, of Minnesota frontier in the 1850s, Civil War, Sioux uprising, etc.\"--Bookseller's catalogue.  \n",
       "174319  In this colorful tale of the food culture of cattle drives in the 1800s, chuckwagon chef Cookie takes young readers along on a rootin'-tootin' adventure. The days start at three o'clock in the morning, when Cookie makes coffee so thick \"you could float a horseshoe on it!\" With informational sidebars, a historical note, bibliography, and glossary for cowboy food terms such as \"calf slobbers\" an...  \n",
       "32008                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
       "46496                                                                              Peggy Simson Curry's memorable short stories, many set on Wyoming and Colorado ranches, originally appeared in magazines during the heyday of short stories in the 1950s-60s. Now they are available in book form. This collection includes two Spur winners and stories from Saturday Evening Post, Boy's Life and other magazines.  \n",
       "93031   The catching of fish, said the Sage of Chocoloskee, is but an incident in fishing. He told the frozen truth. To be out in the open where fish are; to watch them at their great business of living; to see them in the water or out of the water; to fish for them, and even to hook them and have them get away-all this is wonderfully worthwhile-wonderfully better worthwhile than merely to catch and k...  \n",
       "86331   The Kickapoo Indians, members of the Algonquian linguistic community, resisted white settlement for more than three hundred years on a front that extended across half a continent. In turn, France, Great Britain, the United States, Spain, and Mexico sought to placate and exploit this fiercely independent people. Eventually forced to remove from their historic homeland to territory west of the M...  \n",
       "5541    No one today thinks of Brooklyn, New York, as an agricultural center. Yet Kings County enjoyed over two centuries of farming prosperity. Even as late as 1880 it was one of the nation's leading vegetable producers, second only to neighboring Queens County. In Of Cabbages and Kings County, Marc Linder and Lawrence Zacharias reconstruct the history of a lost agricultural community. Their study fo...  \n",
       "59841   From the gentlemen farmers among our founders to todayâ€™s grandchild walking the beans, rural life has always been at the very heart of the American story. This is the life that unfolds page by page in this heartfelt book about working the land. One remarkable photograph after another celebrates the farming life, finding the beauty in work well done, land well tended, and a rest well earnedâ€”all...  \n",
       "84369   The reissue of this classic history allows us to once again journey into the past and rediscover for the first time the forgotten men and methods of logging history in the Northwest United States and Canada. This book contain the best photographs of a dozen famous collections: Davis and Benson rafts, river drives, hand logging spar topping big wheels in the pine, saw mills of 1890 to 1915, his...  \n",
       "879                                                                                                                                                                                                        In 1853, four Scandinavian indentured laborers in Russian Alaska steal a canoe and begin to paddle south toward the mouth of the Columbia River, twelve thousand miles away. Reprint. 15,000 first printing.  \n",
       "141633                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
       "112638                                                                                                                                                                                                                                                                             The story of the development of the farm tractor with more than 250 beautiful photographs of tractors from across the United States.  \n",
       "51225                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
       "203859  Fatty acids and lipids: structures, extraction and fractionation into classes -- Gas chromatography: theoretical aspects and instrumentation -- Preparation of methyl ester and other derivatives -- Gas chromatographic analysis of fatty acid derivatives -- Isolation of fatty acids and identification by spectroscopic and chemical degradative techniques -- Gas chromatography--mass spectrometry and...  \n",
       "30900                                                                                                                                                                                                                                                                                     Photographs show over one hundred years in the history of the St. Louis baseball team, from the late 1800s to the present day  \n",
       "35010                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
       "208570                                                                                                                                                  Take a trip \"south of the border\" with this arrangement of a classic Mexican folk song. Schaum keeps the piece at the late elementary level, even though it sounds quite a bit harder. This piece is a crowd-pleaser, effective for festivals and competitions.  \n",
       "80100                                                                                                                                                                                                                                                                                                                       Published in 1924, this volume contains various views on taxation within the United States.  \n",
       "15675                                                                                                                                                                                                                                                                                                                                                                                                               NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score                                                                                                                                                                                                                                                                                                                                                                                                                 2.690624\n",
       "Title                                                                                                                                                                                                                                                                                                                                                                                                       George Orwell 1984\n",
       "description    \"Nineteen Eighty-Four: A Novel\", often published as \"1984\", is a dystopian social science fiction novel by English novelist George Orwell. It was published on 8 June 1949 by Secker & Warburg as Orwell's ninth and final book completed in his lifetime. Thematically, \"Nineteen Eighty-Four\" centres on the consequences of totalitarianism, mass surveillance, and repressive regimentation of persons a...\n",
       "Name: 81605, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results.loc[81605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8531                                                                                                                                                     Orwell's London\n",
       "25642                                                                                                                    George Orwell: Animal Farm-Nineteen Eighty-Four\n",
       "34435                                                                  History Will Not Absolve Us : Orwellian Control, Public Denial, & the Murder of President Kennedy\n",
       "37594                                                                                                                                       George Orwell's 1984: A Play\n",
       "57832                                                                                                                                       CliffsNotes on Orwell's 1984\n",
       "74376                                                                                                                                       Orwell (Life & Times Series)\n",
       "81605                                                                                                                                                 George Orwell 1984\n",
       "93375                                                                                                                                                 Why Orwell Matters\n",
       "112408                                                                                             Orwell's Fiction: Burmese Days, A Clergyman's Daughter, Keep the Asp,\n",
       "137499                                                                                                                                             George Orwell: A life\n",
       "141582                                                                                                                       George Orwell's Animal Farm (Bloom's Notes)\n",
       "142086                                                                                                                                                  Orwell: the Life\n",
       "149857      George Orwell: Animal Farm, Burmese Days, A Clergyman's Daughter, Coming Up for Air, Keep the Aspidistra Flying, Nineteen Eighty-Four: Complete & Unabridged\n",
       "168154                                                                                                  The Orwell reader: Fiction, essays, and reportage (Harvest book)\n",
       "168462                                                                                   Totalitarian Language: Orwell's Newspeak and Its Nazi and Communist Antecedents\n",
       "177599    The Mammoth Book of Journalism: 101 Masterpieces from the Finest Writers and Reporters, Including Ernest Hemingway, George Orwell, Martha Gell (Mammoth Books)\n",
       "179997                                                                                                                             Orwell's Revenge. The 1984 Palimpsest\n",
       "183443                                                                                                     The Collected Essays, Journalism and Letters of George Orwell\n",
       "187620                                                                                                                                 Inside George Orwell: A Biography\n",
       "191459                                                                                                                             George Orwell and the origins of 1984\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data[(books_data.Title.str.len()>3) & (books_data.Title.str.lower().str.contains(\"\"))].Title.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                        Harry Potter and The Sorcerer's Stone\n",
       "description      Celebrate 20 years of Harry Potter magic! Harr...\n",
       "authors                                          ['J. K. Rowling']\n",
       "image            http://books.google.com/books/content?id=HksgD...\n",
       "previewLink      http://books.google.com/books?id=HksgDQAAQBAJ&...\n",
       "publisher                                    Bloomsbury Publishing\n",
       "publishedDate                                           2014-01-09\n",
       "infoLink         http://books.google.com/books?id=HksgDQAAQBAJ&...\n",
       "categories                                    ['Juvenile Fiction']\n",
       "ratingsCount                                                   1.0\n",
       "Name: 36776, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data.iloc[36776]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('book_recommender')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98d862ff8fc58d25376058c3f52e24b7feee90c4848732fa1a0474216c84b788"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
